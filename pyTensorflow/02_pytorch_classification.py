# -*- coding: utf-8 -*-
"""02_Pytorch_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ilamzd_8qCmSs8Y2vrEXqbr2q6nEAdCI

# 9:11:00

there are 3 types of it :

1. Binary

2. Multilabel

3. multiclass
"""

from sklearn.datasets import make_circles
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
n_samples = 1000

X,y = make_circles(
    n_samples=n_samples,
    noise=0.03,
    factor=0.2,
    random_state=42
)

y[:10]

tf.random.set_seed(42)
circles = pd.DataFrame({"X0" : X[:,0] , "X1" : X[:,1], "label":y})
circles

plt.scatter(X[:,0],X[:,1],c=y,cmap="bwr")

#Input and output shapes
X.shape,y.shape

from sklearn.model_selection import train_test_split

X_train , X_test, y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42)

tf.random.set_seed(42)

model_0 = tf.keras.Sequential([
      tf.keras.layers.Dense(1)
])

model_0.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.SGD(),
    metrics= ["accuracy"]
  )

model_0.fit(X_train , y_train, epochs = 10)

model_0_pred = model_0.predict(X_test)
model_0_pred[:10]

model_0.fit(X,y , epochs=10,verbose=0 )
model_0.evaluate(X,y)

plt.figure(figsize=(10,7))
plt.scatter(X_test[:,0],X_test[:,1],c=y_test,cmap="bwr")


plt.scatter(X_test[:,0],X_test[:,1],c=model_0_pred )

tf.random.set_seed(42)

model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(1),

    tf.keras.layers.Dense(1)
])

model_1.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.SGD(),
    metrics= ["accuracy"]
)

model_1.fit(X_train,y_train,epochs=100)

#4 evalluate the model

model_1.evaluate(X,y)

"""#Improving the model

1. Add a layer

2. train for more epochs

3. choose different optimizer or etc
"""

tf.random.set_seed(42)

model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(100 , activation="sigmoid"),
    tf.keras.layers.Dense(10 , activation="sigmoid"),

    tf.keras.layers.Dense(1)
])

model_2.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    metrics= ["accuracy"]
)

model_2.fit(X_train,y_train,epochs=100)

model_2.evaluate(X,y)

"""#To visualize our models predictin lets create a funciton `plot_decision_boundary()`  this function will :

* Take in trained model , features (X ) adn labels (y)

* Create a meshgrid of the fit of the different X values

* Make predictions across the meshgrid

* Plot the predictions as a colorbar

#Check this code to improve cz i dont know what its doin
"""

import numpy as np
import matplotlib.pyplot as plt

def plot_decision_boundary(model, X, y):
    """
    Plots the decision boundary of a classification model.

    Args:
        model: The trained classification model.
        X: The feature data.
        y: The target labels.
    """
    # Define the boundaries
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1

    # Create a meshgrid
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))

    # Prepare input for predictions
    x_in = np.c_[xx.ravel(), yy.ravel()]

    # Make predictions
    y_pred = model.predict(x_in)

    # Check for multi-class classification
    if len(y_pred[0]) > 1:
        print("Doing multiclass classification")
        # Reshape predictions for plotting
        y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)
    else:
        print("Doing binary classification")
        # Round predictions to 0 or 1
        y_pred = np.round(y_pred).reshape(xx.shape)

    # Plot the decision boundary
    plt.contourf(xx, yy, y_pred, cmap="RdYlBu", alpha=0.7)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.show() # Add plt.show() to display the plot

plot_decision_boundary(model_2,X_train,y_train)

#The missing piece : linearity

tf.random.set_seed(42)

model_4 = tf.keras.Sequential([
    tf.keras.layers.Dense(100 , activation="relu"),
    tf.keras.layers.Dense(10 , activation="relu"),

    tf.keras.layers.Dense(1)
])

model_4.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    metrics= ["accuracy"]
)

model_4.fit(X_train,y_train,epochs=100)

plot_decision_boundary(model_4,X_train,y_train)

model_4_pred = model_4.predict(X_test)
model_4_pred[:10]

plot_decision_boundary(model_4,X_test,y_test) ,plot_decision_boundary(model_4,X_test,model_4_pred)

#The missing piece : linearity

tf.random.set_seed(42)

model_5 = tf.keras.Sequential([
    tf.keras.layers.Dense(100 , activation="relu"),
    tf.keras.layers.Dense(100 , activation="relu"),
    tf.keras.layers.Dense(100 , activation="relu"),

    tf.keras.layers.Dense(1 ,activation="sigmoid")
])

model_5.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics= ["accuracy"]
)

model_5.fit(X_train,y_train,epochs=100)

model_5_pred = model_5.predict(X_test)

plot_decision_boundary(model_5,X_train,y_train) ,  plot_decision_boundary(model_5,X_test,y_test) , plot_decision_boundary(model_5,X_test,model_5_pred)

"""#Confusion matrix

1:48:00 to 2:10:00

#MultiClass classificaition

when u have more than 2 clases as an option .
"""

from tensorflow.keras.datasets import fashion_mnist

(train_data,train_labels),(test_data,test_labels) = fashion_mnist.load_data()

print(f"training sample :\n {train_data[0]}\n ")
print(f"training labels :\n {train_labels[0]}\n ")

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

train_labels

plt.imshow(train_data[0],cmap="gray")
plt.title(class_names[train_labels[0]])
#every image has a label with them.....with theat label we can access the class name of that produvt

#note : the class_names should be in same order the labels are.....

import random
plt.figure(figsize=(7,7))
for i in range(4):
  ax = plt.subplot(2,2,i+1)
  random_index = random.randint(0,len(train_data))
  plt.imshow(train_data[random_index],cmap="gray")
  plt.title(class_names[train_labels[random_index]])
  plt.axis (False)

"""#Buildig a model

we should remember these following things :

1. Input shape = 28 X 28 ( shape of one image )
2. output shape = 10 ( one per class of clothing)
3. loss function = tf.keras.CategoticalCrossentropy()
4. output layer activation = softmax ( not sigmoid  { bcz sigmoid is for binary as it gives 0 or 1 but here we have multi class classification })


"""

tf.random.set_seed(42)

fashion_mnist_model_0 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)), # it turns our data into a one long vector.....i.e 28*28 to (None,784)
    tf.keras.layers.Dense(14,activation="relu"),
    tf.keras.layers.Dense(14,activation="relu"),
    tf.keras.layers.Dense(10,activation="softmax") # we can also use softmax directly
])

fashion_mnist_model_0.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    # CategoricalCrossentropy used when we expect labels to be provided into a one_hot representation . if we want to use CategoricalCrossentropy to it we need to use tf.one_hot(test_labels /train_lebels ,  depth = 10 ) to every test and train labels.
    #  butif we want to proveide labels as integers we use it......since our train_labels are also in integer format....i.e 9,9,0,1,2,3 etc we used SparseCategoricalCrossentropy not CategoricalCrossentropy

    optimizer=tf.keras.optimizers.Adam(learning_rate= 0.01),
    metrics=["accuracy"]
)

fashion_mnist_model_0_history = fashion_mnist_model_0.fit(train_data,
                                                          train_labels,
                                                          epochs=10 ,
                                                          validation_data=(test_data,test_labels))

fashion_mnist_model_0.evaluate(test_data,test_labels)

fashion_mnist_model_0.summary()

"""#We want out data to be scaled (or normalized ) , this means they like to have the number in the term between 0 and 1


"""

train_data.min() , train_data.max()

# so we will get thr data and divivde it by the max of it

train_data_norm = train_data / 255.0
test_data_norm = test_data/255.0

train_data_norm.min() , train_data_norm.max()

#now build a normalizaition model

tf.random.set_seed(42)

norm_fashion_model_0 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(4,activation="relu"),
    tf.keras.layers.Dense(4,activation="relu"),
    tf.keras.layers.Dense(10,activation="softmax"), # 10 since the number of class labels are 10

    ])

norm_fashion_model_0.compile(
    loss= tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=["accuracy"]
)

#we will use the normalized trained and test data......

norm_fashion_model_0_history = norm_fashion_model_0.fit(train_data_norm,
                                                          train_labels,
                                                          epochs=10 ,
                                                          validation_data=(test_data_norm,test_labels))

norm_fashion_model_0.evaluate(test_data_norm,test_labels)

"""#**Note** : Neural network tend ot prefer data in numerical form as well as scaled/normalized(numbers between 0 and 1 )"""

norm_fashion_model_0.summary()

import pandas as pd

pd.DataFrame(norm_fashion_model_0_history.history).plot(title="normalized data history")

pd.DataFrame(fashion_mnist_model_0_history.history).plot(title="non normalized data history")

"""# **Note** : the same model with even slightly different data can produce dramaticallly different results . so h=when you are comparing model sthem on the same criteria ( eg same architecture but differeent data or same data but differenet architrcture )

#Since we got a better model ....we will set tje learning rate for this better model
"""

tf.random.set_seed(42)

norm_fashion_model_2 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(4,activation="relu"),
    tf.keras.layers.Dense(4,activation="relu"),
    tf.keras.layers.Dense(10,activation="softmax")
])

norm_fashion_model_2.compile(
    loss= tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=["accuracy"]
)

# Create the learning rate callback

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch : 1e-3 * 10**(epoch/20))

# it means , it uses lambda function ( i think )  , to start the value from 1e-3 epochs and increase every epoch by 10^(epoch/20)

norm_fashion_model_2_history = norm_fashion_model_2.fit(train_data_norm,
                                                          train_labels,
                                                          epochs=10 ,
                                                          validation_data=(test_data_norm,test_labels),
                                                          callbacks=[lr_scheduler])

lrs = 1e-3 * (10**(tf.range(10)/20))

plt.figure(figsize=(10,7))
plt.semilogx(lrs,norm_fashion_model_2_history.history["loss"])
plt.xlabel("learning rate")
plt.ylabel("loss")
plt.title("learning rate vs loss")
plt.ylabel

#lets refit the model with the ideal learning eate we got from it

tf.random.set_seed(42)

norm_fashion_model_3 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(4,activation="relu"),
    tf.keras.layers.Dense(4,activation="relu"),
    tf.keras.layers.Dense(10,activation="softmax")
])

norm_fashion_model_3.compile(
    loss= tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=["accuracy"]
)


norm_fashion_model_3_history = norm_fashion_model_3.fit(train_data_norm,
                                                          train_labels,
                                                          epochs=10 ,
                                                          validation_data=(test_data_norm,test_labels),
                                                          )

"""# Evaluating our multi class classififcation model

To evaluate our multi class classification model we could :

* Evaluate its performance using t=otehr classification metrdics such as confusion metrics

* improve some of its result by training it for longer

* Asses some its predicitons through visualization

* Save and export it for use in appliacatoin

#**Watch from 3:16:00 to 3:30:00**

# **Note*** : often when working with images and other form of visual data itsa  good isea to visualize as much as possoible to develop further of the understanding of data and the inputs and output of uout model

How about we create a fun little function for ;

* plot a random image

* make a predcitoin on said image

* label the plot with the truth label and predicted data
"""

import random

def plot_random_image(model, images, true_labels, classes):
    """Picks a random image, plots it and labels it with a prediction and truth label."""

    # Set up a random number
    i = random.randint(0, len(images))

    # Create predictions and targets
    target_image = images[i]
    pred_probs = model.predict(target_image.reshape(1, 28, 28))
    pred_label = class_names[pred_probs.argmax()]
    true_label = class_names[true_labels[i]]  # Access the true label for the selected image

    # Plot the image
    plt.imshow(target_image, cmap="gray")

    # Change the color of the title depending on whether the prediction is right or wrong
    if pred_label == true_label:  # Compare with the true label of the selected image
        color = "green"
    else:
        color = "red"

    # Add xlabel information (prediction/true label)
    plt.xlabel("Pred: {} {:2.0f}% (True: {})".format(pred_label,
                                                     100 * tf.reduce_max(pred_probs),
                                                     true_label),  # Display the true label for the selected image
               color=color)  # Set the color to green or red based on if the prediction is right or wrong

plot_random_image(model=norm_fashion_model_3,
                  images= test_data_norm, # always make prediction on the same kind of data it was trained on , since this model was trained on normalised data so wwe will use normalised test data to test
                  true_labels=test_labels,
                  classes=class_names)

"""#SO WHAT OUR MODEL IS ACTUALLY LEARNING ??"""

norm_fashion_model_0.layers

norm_fashion_model_0.layers[1]

#get the patterns of a layer in our network

weights , biases = norm_fashion_model_3.layers[1].get_weights()

weights , weights.shape # shape of it corresponds to out image 28*28 , 4 which is number of hidden units or neurons in our hidden layer


# 4 , it means our weights matrix has 4 numbers it starts to learn and adjust to find patterns in this 784 ( 28*28 ) numbers

#each value in weights matrix  corresponds to how a particular value in the dataset should influence the network decision

#Initially the model gives somes random numbers to its weights.....it does it using layer's kernel_initialzer's glorot_uniform which is a kind of randomness

#Now lets check out the **bias** vector

biases = norm_fashion_model_3.layers[1].bias.numpy()
biases , biases.shape

# A wights matrix has 1 value per data point and bias vector has  1 value per hidden unit or neuron of hidden layer

"""# every neuron has a bias vector of its own . Each of these is paired witha a weight matrix

The bias vecotr get initialized as zeros ( atleast in the case of a tensorflow dense layer )

The bias vector dictates how much the pattern within the corresponding wieights matrix should influence the next layer
"""

#lets refit the model with the ideal learning eate we got from it

tf.random.set_seed(42)

norm_fashion_model_4 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(10,activation="relu"),
    tf.keras.layers.Dense(10,activation="relu"),
    tf.keras.layers.Dense(10,activation="softmax")
])

norm_fashion_model_4.compile(
    loss= tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=["accuracy"]
)


norm_fashion_model_4_history = norm_fashion_model_4.fit(train_data_norm,
                                                          train_labels,
                                                          epochs=10 ,
                                                          validation_data=(test_data_norm,test_labels),
                                                          )

plot_random_image(model=norm_fashion_model_4,
                  images= test_data_norm, # always make prediction on the same kind of data it was trained on , since this model was trained on normalised data so wwe will use normalised test data to test
                  true_labels=test_labels,
                  classes=class_names)

weights , biases = norm_fashion_model_4.layers[1].get_weights()
# it has 10 neurons in each hidden layer
weights , weights.shape

