# -*- coding: utf-8 -*-
"""01_NN_tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKw8xPerStn88iDSyfk6jTor9BCiYTY0
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

#create features
X = np.array([-7.0,-4.0 , -1.0 , 2.0, 5.0 , 8.0 , 11.0 , 14.0])

#create labels

y = np.array([3.0 , 6.0 , 9.0 , 12.0 ,15.0, 18.0,21.0 , 24.0])

plt.scatter(X,y)

y == X+10

"""#Input and output shapes"""

#create a demo tensor for out housignprediction problem

house_info  = tf.constant(["bedroom","bathroom" , "garage"])  # we are using house_info to get house_price as output
house_price = tf.constant([1000])
house_info , house_price

input_shape= X.shape
output_shape = y.shape
input_shape , output_shape

X[0].shape # is an scalar i.e 1.0 is scalar do .ndim......like we are taking 1 X value to predict 1 y value

#turn numpy into tensor

X = tf.constant(X)
y= tf.constant(y)
X,y

input_shape= X[0].shape
output_shape = y[0].shape
input_shape , output_shape

"""#Steps in moddelling wiith tensorflow

1. **creating the mode** - define the input and output layer as well as hidden layers of a deep learning model

2. **compiling a mode** - define the loss finction ( in other words the finction which tells out model how wrong it is)  and the optimizer ( tells our model ohow to improve the pattern its learning )  and evaluation metrics (what we can use to interpret the performance of our model)

3. **Fitting of a model** - letting the model try to find patterns between X and y ( features and labels )
"""

# X = tf.squeeze(X)
# X.shape

X = tf.expand_dims(X, axis=-1) # i have to use this bcz my X was squeezed so i have to create a

X.shape

tf.random.set_seed(42)

#1. create a model using the Sequential API
# ( i.e creating a model and telling him to go through the layers one by one sequentially usign the Sequential API)

model = tf.keras.Sequential([
    # tf.keras.layers.Dense(100, activation= "relu"),

    tf.keras.layers.Dense(1)

    #Dense is basicaally FULLLY CONNECTED LAYER means each neuron is connected with every other neuron of next layer
])

#2. compilee the model

model.compile(loss = tf.keras.losses.mae ,# mae means mean absolute error
              optimizer = tf.keras.optimizers.SGD(), # stochastic gradient descent
              metrics = ["mae"]  # it is human interpretable values for how well your model is doing
              )


#3. fit the model

model.fit(X,y,epochs = 10)

#4. Evaluate the model on the test data to check how reliable our model is

"""#we can add layers in our model either as making them iin a list like above or using add method

model = tf.keras.Sequential()

model.add(tf.keras.layers.Dense(100,
activation= "relu"))

model.add(tf.keras.layers.Dense(10))

model.add(tf.keras.layers.Dense(1))
"""

#prediction using out model
# model.predict([17.0]) mine shows error but sirs dont so we have to conevrt it to numpy first
model.predict(np.array([17.0])) # we cannot directly put tensors in it so we have to conert it into array first

"""# Improving the model

we can improve our model by altering the steps we took to create a model

1. **Creating a model** -  here we might add more layers to inc the number of hidden units called neurons within each o the hidden layers , change the activation of each layer

2. **Compiling  a model** - here we might change the optimization function or perhaos the **learning rate** of the optimiztion function

3. **fiiting a model** -  here we might fit a model for more epocs leaving it for training for longer or on more data ( given the model more examples to learn from)
"""

X=tf.expand_dims(X,axis=-1)
y=tf.expand_dims(y,axis=-1)

tf.random.set_seed(42)

#1. create a model using the Sequential API
# ( i.e creating a model and telling him to go through the layers one by one sequentially usign the Sequential API)

model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation= "relu" , name="input_layer"),
    tf.keras.layers.Dense(1, name="output_layer")
] ,name="model_1" )

#2. compilee the model

model_1.compile(loss = tf.keras.losses.mae ,# mae means mean absolute error
              optimizer = tf.keras.optimizers.Adam(learning_rate=0.1),
              metrics = ["mae"]  # it is human interpretable values for how well your model is doing
              )


#3. fit the model

model_1.fit(X,y,epochs = 100)

#4. Evaluate the model on the test data to check how reliable our model is

 # now it is basically overfitting........

tf.squeeze(X)
X.shape

model_1.predict(np.array([17.0]))

"""#Evaluating a model


In practice , a typical workflow you wil l fo through when building a neurak network is :

``` Build a model -> fit it -> evaluate it -> tweak it -> fit it ->evaluate it ->tweak -> fit -> eval - >repeat ```

its what we did it in preevious model

when it comes to evaluation ......there is only VISUALIZE

IN GENERAL IT MEANS :

What data are we working with ?

what does our model look like ?

how does a model perform while it learns ??

How the preductions are done ???
"""

X = tf.range(-100,100,4)

y = X+10

plt.scatter(X,y)

# from sklearn import model_selection
# from sklearn.model_selection import train_test_split

# X = tf.range(-100,100,4).numpy() # convert the set to numpy first before using in train_test_split
# y = X+10
# # y=y.numpy()

# X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 42)

# X_train.shape , X_test.shape , y_train.shape , y_test.shape

# or we can use
tf.random.set_seed(42)

X_train = X[:40]
y_train =y[:40]

X_test = X[40:]
y_test = y[40:]

X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

y_train = np.expand_dims(y_train, axis=-1)
y_test = np.expand_dims(y_test, axis=-1)

X_train.shape , X_test.shape , y_train.shape , y_test.shape

plt.figure(figsize=(10,7))
plt.scatter(X_train,y_train,c="b",label="training data")
plt.scatter(X_test,y_test,c="g",label="testing data")
plt.legend()



tf.random.set_seed(42)

model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])

model_2.compile(loss = tf.keras.losses.mae,
                optimizer = tf.keras.optimizers.SGD(),
                metrics= ["mae"])

model_2.fit(X_train , y_train , epochs = 100)

# X_train is training data features or inputs and y_train is the labels or output of the training .....train data and train label ......train input and train output

y_pred = model_2.predict(np.array(X_test))


plt.figure(figsize=(10,7))
plt.scatter(X_train,y_train,c="b",label="training data")
plt.scatter(X_test,y_test,c="g",label="testing data")
plt.scatter(X_test,y_pred,c="r",label="testing data")

X_new = tf.constant(X)
y_new = tf.constant(y)

X,y

# or we can use.....so its better to covert the data to tensor first before using them......
tf.random.set_seed(42)

X_train = X_new[:40]
y_train =y_new[:40]

X_test = X_new[40:]
y_test = y_new[40:]

X_new_train = np.expand_dims(X_train, axis=-1)
X_new_test = np.expand_dims(X_test, axis=-1)

y_new_train = np.expand_dims(y_train, axis=-1)
y_new_test = np.expand_dims(y_test, axis=-1)

X_new_train.shape , X_new_test.shape , y_new_train.shape , y_new_test.shape

X_new_train =tf.constant(X_new_train)
X_new_test = tf.constant(X_new_test)
y_new_train = tf.constant(y_new_train)
y_new_test = tf.constant(y_new_test)

X_new_train , X_new_test , y_new_train , y_new_test

#ltes create a model which builds automatically by defining the input_shape argument in the first layer
tf.random.set_seed(42)

model_3 = tf.keras.Sequential([
    tf.keras.layers.Dense(1, activation= "relu"),

])

model_3.compile(loss = tf.keras.losses.mae ,# mae means mean absolute error
                optimizer = tf.keras.optimizers.Adam(),
                metrics = ["mae"]  # it is human interpretable values for how well your model is doing
                )


model_3.fit(X_new_train,y_new_train,epochs = 100)

y_new_pred = model_3.predict(X_new_test)

# using tensor i didnt need it to use np.array()

plt.figure(figsize=(10,7))
plt.scatter(X_new_train,y_new_train,c="b",label="training data")
plt.scatter(X_new_test,y_new_test,c="g",label="testing data")
plt.scatter(X_new_test,y_new_pred,c="r",label="testing data")

X_new_test

#Lets create  amodel which builds automatical by defiining the input_shape argument in the layer


tf.random.set_seed(42)


model_4 = tf.keras.Sequential([
    tf.keras.layers.Dense(1,
                          input_shape=[1]),

])

model_4.compile(loss = tf.keras.losses.mae,
                 optimizer= tf.keras.optimizers.SGD(),
                 metrics = ["mae"]
 )

model_4.fit(X_new_train,y_new_train,epochs = 100)

X[0]

model_1.summary()

model_4.summary()
# Dense is FULLY CONNECTED NETWROK
#trainable parans are the parameters the model can update as it trains
#non-trainanble params arent updated during TRAINING ( this is typically when u bring in already learn patterns or parameters from other mdoels during transfer learning)

"""#watch https://youtu.be/tpCFfeUEGs8?si=SkwRkLoy6Sfo2x_F from 5:45:00 for about these params thing"""

X

X_train , X_new_train

model_1.summary()

from tensorflow.keras.utils import plot_model

plot_model(model=model_1, show_shapes=True)
# we  can also name our layer byy putting....name="layername on it"

"""#Visualize

to visualize predictions , its good idea to plot them against the ground truth labels

often you will see in the form of `y_test` `y_pred`

and i already have done it in before model so practice them
"""

def plot_predictions(
    train_data=X_train,
    train_labels=y_train,
    test_data=X_test,
    test_labels=y_test,
    predictions=y_pred

    ):

  plt.figure(figsize=(10,7))
  plt.scatter(train_data,train_labels,c="b",label="training data")
  plt.scatter(test_data,test_labels,c="g",label="testing data")
  plt.scatter(test_data,predictions,c="r",label="predictioins")
  plt.legend()

plot_predictions(X_new_train,y_new_train,X_new_test,y_new_test,y_new_pred)

"""### Evaluating our models predictions with the regression evalutaion metrics

Depending on the problems you are working on there willl be different evaluation mdetrics to evvaluate your models performance

Since we are working on a regression , two of the main metrics :

* MAE - mean absolute error , on average  ,how wrong is each of my models prediction

* MSE - mean square error , on average , how wrong is each of my models prediction squared  


"""

model_1.evaluate(X_new_test,y_new_test)

y_new_pred

import tensorflow as tf

# Calculate MAE using the functional API
mae = tf.keras.metrics.MeanAbsoluteError() # initialize the class
mae.update_state(y_true=y_new_test, y_pred=y_new_pred) # update the class with ground truth and predicted values
mae = mae.result().numpy() # Get the result

# Or, create a metric object and update its state
# This part is redundant as it does the same thing as the previous code
# mae_metric = tf.keras.metrics.MeanAbsoluteError()
# mae_metric.update_state(y_new_test, y_new_pred)
# mae = mae_metric.result().numpy()

print(mae)  # Print the MAE value

import tensorflow as tf

# Use tf.keras.metrics.MeanSquaredError for calculating MSE as a metric
mse = tf.keras.metrics.MeanSquaredError()
mse.update_state(y_new_test, y_new_pred)
mse = mse.result().numpy()
print(mse)

"""### Running experiments to improve our model

1. Get more data - get more examples for ur model to train on

2. Make yout model larger using a more complex modeel - this migh tcome in form of layers or more hidden units in each layer

3. train for longer -= give ur model more chance to find patterns in the data

Lets do 3 modelling experiments ‚Åâ

1. `model` - same as the origina model , 1 layer , trained for 100 epochs

2. `model_1` - 2 layers , trained for 100 epochs

3. `model_5` - 2 layers , trained for 500 epochs

#Build new model from start
"""

A = tf.range(-100,100,4)
A

tf.random.set_seed(42)

A_train = X[:40]
b_train =y[:40]

A_test = X[40:]
b_test = y[40:]

# A_train = np.expand_dims(X_train, axis=-1) only run them 1 time
# b_test = np.expand_dims(X_test, axis=-1)

# b_train = np.expand_dims(y_train, axis=-1)
# b_test = np.expand_dims(y_test, axis=-1)

A_train.shape , A_test.shape , b_train.shape , b_test.shape



tf.random.set_seed(42)

new_model_0 = tf.keras.Sequential([
    tf.keras.layers.Dense(1)
] ,name="new_model_0" )

#2. compilee the model

new_model_0.compile(loss = tf.keras.losses.mae ,
              optimizer = tf.keras.optimizers.SGD(),
              metrics = ["mae"]
              )


#3. fit the model

new_model_0.fit(A_train,b_train,epochs = 100)

new_y_pred_0 = new_model_0.predict(A_test)
plot_predictions(A_train,b_train,A_test,b_test,new_y_pred_0)

tf.random.set_seed(42)

new_model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(1)
] ,name="new_model_1" )

#2. compilee the model

new_model_1.compile(loss = tf.keras.losses.mae ,
              optimizer = tf.keras.optimizers.SGD(),
              metrics = ["mae"]
              )


#3. fit the model

new_model_1.fit(A_train,b_train,epochs = 200)

new_y_pred_1 = new_model_1.predict(A_test)
plot_predictions(A_train,b_train,A_test,b_test,new_y_pred_1)

tf.random.set_seed(42)

new_model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(1)
] ,name="new_model_1" )

#2. compilee the model

new_model_2.compile(loss = tf.keras.losses.mae ,
              optimizer = tf.keras.optimizers.SGD(),
              metrics = ["mae"]
              )


#3. fit the model

new_model_2.fit(A_train,b_train,epochs = 500)

new_y_pred_2= new_model_2.predict(A_test)
plot_predictions(A_train,b_train,A_test,b_test,new_y_pred_2)

tf.random.set_seed(42)

new_model_3 = tf.keras.Sequential([
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
] ,name="new_model_3" )

#2. compilee the model

new_model_3.compile(loss = tf.keras.losses.mae ,
              optimizer = tf.keras.optimizers.SGD(),
              metrics = ["mae"]
              )


#3. fit the model

new_model_3.fit(A_train,b_train,epochs = 100)

new_y_pred_3= new_model_3.predict(A_test).squeeze()
plot_predictions(A_train,b_train,A_test,b_test,new_y_pred_3)

tf.random.set_seed(42)

new_model_4 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(1)
] ,name="new_model_4" )

#2. compilee the model

new_model_4.compile(loss = tf.keras.losses.mae ,
              optimizer = tf.keras.optimizers.SGD(),
              metrics = ["mae"]
              )


#3. fit the model

new_model_4.fit(A_train,b_train,epochs = 100)

new_y_pred_4= new_model_4.predict(A_test).squeeze()
plot_predictions(A_train,b_train,A_test,b_test,new_y_pred_4)

"""#comparing thr result of our experiments"""

model_0_avg_error = tf.reduce_mean(tf.abs(new_y_pred_0 - b_test))
model_1_avg_error = tf.reduce_mean(tf.abs(new_y_pred_1 - b_test))
model_2_avg_error = tf.reduce_mean(tf.abs(new_y_pred_2 - b_test))
model_3_avg_error = tf.reduce_mean(tf.abs(new_y_pred_3 - b_test))
model_4_avg_error = tf.reduce_mean(tf.abs(y_new_pred_4 - b_test))

model_1_avg_error.numpy(), model_2_avg_error.numpy(), model_3_avg_error.numpy(), model_4_avg_error.numpy()

# Calculate MAE for each model separately

model_0_mae = tf.keras.metrics.MeanAbsoluteError()
model_0_mae.update_state(y_true=b_test, y_pred=new_y_pred_0)
model_0_mae = model_0_mae.result().numpy()

model_1_mae = tf.keras.metrics.MeanAbsoluteError()
model_1_mae.update_state(y_true=b_test, y_pred=new_y_pred_1)
model_1_mae = model_1_mae.result().numpy()

model_2_mae = tf.keras.metrics.MeanAbsoluteError()
model_2_mae.update_state(y_true=b_test, y_pred=new_y_pred_2)
model_2_mae = model_2_mae.result().numpy()

model_3_mae = tf.keras.metrics.MeanAbsoluteError()
model_3_mae.update_state(y_true=b_test, y_pred=new_y_pred_3)
model_3_mae = model_3_mae.result().numpy()

model_4_mae = tf.keras.metrics.MeanAbsoluteError()
model_4_mae.update_state(y_true=b_test, y_pred=new_y_pred_4)
model_4_mae = model_4_mae.result().numpy()

model_0_mae, model_1_mae, model_2_mae, model_3_mae, model_4_mae

import pandas as pd

model_results = [["new_model_0" ,model_0_mae],
                 ["new_model_1",model_1_mae],
                 ["new_model_2",model_2_mae],
                 ["new_model_3",model_3_mae],
                 ["new_model_4",model_4_mae]
                 ]

all_model_results = pd.DataFrame(model_results,columns=["model","mae"])
all_model_results

"""#Saving our Models

There are 2 main formats we can save our models too :

1. The SavedModel format
2. The HDF5 format


"""

new_model_1.save("Best_regression_model_SavedModel_format.keras")  # read the documentation too about what does it really saves in model and what we can retrieve when we load it
 # Best_regression_model is the file path we wanna save to

#2 HDF5

new_model_1.save("Best_regression_model_HDF5_format.h5")

"""#Loading the saved model"""

#laod in the savedmodel format

loaded_SavedModel_format=  tf.keras.models.load_model("/content/Best_regression_model_SavedModel_format.keras")

loaded_SavedModel_format.summary()

#compare new_model_1 predictions with the SavedModel fomrat modl predicitons just to check whether they are correct or not


new_model_1_preds =  new_model_1.predict(A_test)
loaded_SavedModel_format_preds = loaded_SavedModel_format.predict(A_test)
new_model_1_preds == loaded_SavedModel_format_preds

#2 load H5 model

loaded_H5_format=  tf.keras.models.load_model("/content/Best_regression_model_HDF5_format.h5")

loaded_H5_format.summary()

#and same compare them to check if both are same or not as before

#Download a file from google collab or check 7:25:00

# from google.colab import files

# files.download("/content/Best_regression_model_SavedModel_format.keras")

"""# 7:25:00 - 8:10:00 are recap so watch it if u want

#Preprocessing ( normalization and standardization)

**Normalization** - a technoque yo change the value of numeric coloumns in the dataset to common sacle , without distorting differences in the range of values
or
converting the value between 0 and 1 while preserving the original distribution

**Standardization** - removes the mean and divides each value by standard deviation .......it transform a feature to have close t normal distrivution ( caution : this reduces the effect of outlier )
"""

import pandas as pd

insurance = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/refs/heads/master/insurance.csv")
insurance.head()

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler,OneHotEncoder

#create a column transformer

ct =make_column_transformer(
    (MinMaxScaler(),["age","bmi","children"]), # turn all values in these values in these columns between 0 and 1
    (OneHotEncoder(handle_unknown="ignore"),["sex","smoker","region"])
)

#create X and y

C = insurance.drop("charges",axis=1)
D = insurance["charges"]

#build our train and test sets

from sklearn.model_selection import train_test_split

C_train , C_test , D_train , D_test = train_test_split(C,D,test_size=0.2,random_state=42)

#fit the column transformer to our training data

ct.fit(C_train)

#Transforming training and testing data with the normaliztoin (MinMaxScaler ) and OneHotEncoder

C_train_normal = ct.transform(C_train)
C_test_normal = ct.transform(C_test)

# Access the first row of C_train using .iloc
C_train_normal[0], C_train.iloc[0]

tf.random.set_seed(42)

insurance_model_4 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

insurance_model_4.compile(loss = tf.keras.losses.mae,
                        optimizer = tf.keras.optimizers.Adam(),
                        metrics = ["mae"])

insurance_model_4.fit(C_train_normal,D_train,epochs = 100)

insurance_model_4.evaluate(C_test_normal,D_test)

insurance_model_4_preds = insurance_model_4.predict(C_test_normal)

insurance_model_4_preds.shape , C_test_normal.shape

#evaluate our insurance model trained on normalized data

insurance_model_4.evaluate(C_test_normal,D_test)

insurance_model_4_preds = insurance_model_4.predict(C_test_normal)

plt.figure(figsize=(10,7))
# We want to plot the predicted values against the actual values
# so X_axis will be D_test (Actual values)
# and Y_axis will be insurance_model_4_preds (Predicted values)
plt.scatter(D_test, tf.squeeze(insurance_model_4_preds))  # Changed this line
# Plot a line representing perfect predictions
plt.plot([D_test.min(), D_test.max()], [D_test.min(), D_test.max()], 'k--', lw=2)  # Added this line
plt.xlabel("Actual Charges")
plt.ylabel("Predicted Charges")
plt.title("Actual vs Predicted Charges")
plt.show()

