# -*- coding: utf-8 -*-
"""Data_augmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AJtPhoVUnDALfZALcIwOYPSCJyb0lIdi
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import cv2
import tensorflow as tf
import PIL
import os

from tensorflow import keras
from tensorflow.keras  import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D

from sklearn.model_selection import train_test_split

data_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos',cache_dir='.' ,origin=data_url, untar=True)

data_dir

# to convert data_dir into pathlib  use pathlib....its a Windows path through which we can access all the images in each directory

import pathlib
from pathlib import Path

data_dir = Path('./datasets/flower_photos/flower_photos')
# if we dont do it then we would have to use ** to go to every subdirectory

data_dir = pathlib.Path(data_dir)
data_dir

images  = list(data_dir.glob('*/*.jpg')) # means give me path of every data which has .jpg extension ** means go to every subdirectory / * means any image with .jpg extension
len(images) , images[:5]

roses= list(data_dir.glob('roses/*')) # go to directory named roses in data_dir and give every file in it or we can use .jpg to get every jpg file only
len(roses)

tulips= list(data_dir.glob('tulips/*')) # go to directory named roses in data_dir and give every file in it or we can use .jpg to get every jpg file only
len(tulips)

PIL.Image.open(roses[40]) # it will show us image

# so we will create a dictionary.....the dictinory will gave roses or flower name as key and their path as value and that value will be in form of list

flower_images_dict = {
    'roses': list(data_dir.glob('roses/*')), # it will give us list of all the roses imgae , the list will be in form of array
    'daisy': list(data_dir.glob('daisy/*')),
    'dandelion': list(data_dir.glob('dandelion/*')),
    'sunflowers': list(data_dir.glob('sunflowers/*')),
    'tulips': list(data_dir.glob('tulips/*')),
}

flower_images_dict['roses'][:5] # so each flower will have all the file path for each of their image categories

# now we will give them label since we cant use roses as name

flower_label_dict = {
    'roses' : 0,
    'daisy':1,
    'dandelion':2,
    'sunflowers':3,
    'tulips':4,
}

img  =  cv2.imread(flower_images_dict['roses'][0]) # here we will give a supply path and in return it will gives us numpy array of it since we need to convert image in numer

img # so cv2 took the image and converted it into 3 dimensional numpy array into 3 colour channels

#since our image dimension are so varying we want to make the dimesion of all the images same to train our model for it , to do it we will use resize

cv2.resize(img,(180,180)).shape

X, y =[] , []

for flower_name, images in flower_images_dict.items():
  # here flowe_name is key and images is the by default value of it...its all by default when we go in a loop in a dictionary......

    for image in images:
        img = cv2.imread(str(image)) # open the image and convert it into numpy array cz we cant resize it firectly and store in img so we can resize
        resized_img = cv2.resize(img,(180,180)) # now reesize it
        X.append(resized_img) # append the str of this resized image in X .....like X contain the numpy array of it.....
        y.append(flower_label_dict[flower_name])

X[0] , y[0]

# for convienence we will convert it into numpy array bcz X contains a list and we converted X into numpy array

#before  Each item in the list is a numpy array (image)

# Convert X and y to numpy arrays
X = np.array(X)  # X is a list of images (each image is a numpy array)
y = np.array(y)  # y is a list of labels (flower categories)

#  X is now a numpy array of shape (num_images, height, width, channels)

X_train , X_test , y_train , y_test = train_test_split(X,y , test_size=0.3 , random_state=42)

len(X_train)

#normalize/scale the train and test .....so that the data in numpy array can be in b/w 0 and 1
X_train_scaled = X_train/255
X_test_scaled = X_test/255

# we didnt scaled y bcz y only has 5 labels in it , so no need for now

model = Sequential([
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(32,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

     layers.Conv2D(64,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

     layers.Flatten(),
    layers.Dense(128,activation='relu'),
    layers.Dense(64,activation='relu'),
    layers.Dense(32,activation='relu'),
    layers.Dense(5) # 5 neurons as final output bcz there are totoal 5 flower type, so it is pretty clear that we want to clasify on basis of flower
    # we can use softmax at output as activaiton to get result from numpy array to a set of probablilites which is basically b.w 0 and 1 as probabilites can be b/w 0 and 1

])


model.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

history = model.fit(X_train_scaled,y_train, validation_split=0.1, epochs=10)

model.evaluate(X_test_scaled,y_test)
# there is 65% test accuracy which is bad and this is happening bcz of OVERFITTING bcz the training has 98% accuracy causing overfitting which in then making our test accuracy bad/very low......

# we can use data augmentation to resolve this

predictions = model.predict(X_test_scaled)

#since the prediciton are in probability score i.e 0 and 1 se we will use argmax function to find the maximum amng all these 5 neurons.......

# like lets say it generated an output prediciton and each sample has probabilities of each neuron which is 5 bcz there are 5 classes......so basically each prediction sample will have a probility of that test sample having probability of being most similiar to that neuron

# liek.....prediction[0] = [0.11113 , 0.3443322 , 0.33566622, 0.7777633, 0.6666633223]
# so these are probilities of each neuron which is basically each flower repectivrly
# now we want to find which one is greatest and find the correspondng label of it....which is basically the label value of it.....

score = tf.nn.softmax(predictions[00]) # bcz softmax converts predictions[0] into a value b.w 0 and 1

np.argmax(score) , y_test[00] # for both of belinged to same flower class

from tensorflow.keras import layers

data_augmentation = keras.Sequential([

    layers.RandomZoom(0.3),
    layers.RandomRotation(0.1),
    layers.RandomFlip('horizontal')

    # and so on, there are multiple Random** layers available for augmentation
])

plt.axis('off')
plt.imshow(X[0])

plt.axis('off')
plt.imshow(data_augmentation(X)[0].numpy().astype("uint8")) # converrt it into numpy array
# we have to call i like a funciton ....

# so here we will apply a data augmentation at first layer and can also use Dropout() to dropout number of neurons in after everry layer after each pass at random , fo better generalization

# The layers.Dropout(0.2) is used to add a Dropout layer to your neural network model in Keras. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of the input units to 0 during training.

# 0.2: This represents the dropout rate, i.e., 20% of the units in the layer will be randomly set to 0 during training.
# This means that each time the model is trained, 20% of the neurons in this layer will be "dropped" or ignored, forcing the network to learn redundant representations and thus improving generalization.

model_1 = Sequential([
    data_augmentation,
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(32,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

     layers.Conv2D(64,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

    layers.Dropout(0.2), # drop 20 % neurons after it ( i think )

     layers.Flatten(),
    layers.Dense(128,activation='relu'),
    layers.Dense(64,activation='relu'),
    layers.Dense(32,activation='relu'),
    layers.Dense(5,activation="softmax")

])


model_1.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy']
)

history = model_1.fit(X_train_scaled,y_train, validation_split=0.1, epochs=10)

model_1.evaluate(X_test_scaled,y_test)

predictions = model_1.predict(X_test_scaled)

predict = (np.argmax(predictions)==y_test)
predict

predict = predict.astype(int)

match = 0
unmatch = 0

for value in predict:  # âœ… Iterating over values directly
    if value == 1:
        match += 1
    else:
        unmatch += 1

match , unmatch

