# -*- coding: utf-8 -*-
"""tensorflow_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1stCsvEAFOsm4FMQkr2Y7GnjgBrV-6YF0
"""

!pip install tensorflow-text

!pip install tensorflow tensorflow_hub tensorflow_text --upgrade

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as tf_text

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

from sklearn.metrics.pairwise import cosine_similarity

df = pd.read_csv('Spam_SMS.csv')

df.head(5)

df.groupby('Class').describe()

747/4827
# so there are 15% spam emails onlt so we will now do downsampling , to disregard some ham emails so that we can equalize the ratio of spam and ham emails......

# split spam and ham email
df_spam = df[df["Class"]=='spam']
df_ham = df[df["Class"]=='ham']
df_spam.shape , df_ham.shape

df_spam.shape[0] # means 1st outside dimension

df_ham_downsampled = df_ham.sample(df_spam.shape[0])
df_ham_downsampled.shape

#now concatt df_han_downsampled with df_spam

df_balanced = pd.concat([df_spam,df_ham_downsampled])
df_balanced.shape

#now create a new spam column

df_balanced['spam'] = df_balanced['Class'].apply(lambda x: 1 if x=='spam' else 0)
df_balanced.sample(5)

X_train, X_test, y_train, y_test = train_test_split(df_balanced['Message'],df_balanced['spam'], stratify=df_balanced['spam'])

bert_preprocess = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
bert_encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4")

def get_sentence_embeding(sentences):

    preprocessed_text = bert_preprocess(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']
    # it bert_encoder returns an dictionary in wich pooled_output is a key , we can see multiple keys of this dict too

# get_sentence_embeding(["hellow worlf how are thou soul"])

e = get_sentence_embeding([
    "banana",
    "grapes",
    "mango",
    "jeff bezos",
    "elon musk",
    "bill gates"
])

e # e is embedding of all these words

cosine_similarity([e[0]] , [e[1]]) # it expects 2 dimensional list thats why we put [e[0]] in it

cosine_similarity([e[0]] , [e[3]])
# the cosine similiarty shows the similarity between both by using the wikipedia text.......

cosine_similarity([e[5]] , [e[3]])

#bert layers

text_input = tf.keras.layers.Input(shape=(10,), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)
#neural network layers

l = tf.keras.layers.Dropout(0.1, name="dropout")(output['pooled_output'])
# the Dropout will take the output of bert_encoder with pooling_output key as input of this layer

tf.keras.Dense(1,activation="sigmoid")(l) # 1 bcz it would be spam or not spam

#construvt fianl model

model = tf.keras.Model(inputs=[text_input] ,outputs=[l] )

model.summary()

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.evaluate(X_test , y_test)

y_predicted = model.predict(X_test)
y_predicted = y_predicted.flatten()