# -*- coding: utf-8 -*-
"""00_PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k9W8d6YC3vP40AU-3YNl_6tGss5F2y8t

00_PyTorch
"""

print("Heello")

import torch
import pandas as pd
import matplotlib.pyplot as mt
import numpy as np

print(torch.__version__)

"""tensors are way to represent numeric data or data

#Introduction to Tensors

"""

#scalar
#  pytorch tensors are created using torch.tensor()
# torch.tensor is kinda a common lib to create tensors
#the tensor are kinda datatypes.....like this tensor(7)
# the scalar in general has only magnitude ie a number only
#the dimension can be counted on number of squares either closing or opening ones [[[ ]]] has 3 opening and 3 closing brackets means it has 3D one
scalar= torch.tensor(7)
scalar

# a scalar has no dimensioms its just a number or has only 1 number

scalar.ndim #gives the dimension of the scalar ....bscially number of dimensions

#get tensor back as python int

scalar.item() # if we dont want the data as tensor we can use scalar.item to get the integer part only

#we ususally represemnt scalar and vector in term of smaall case words
# we use CAPITAL letter to represemnt TENSORS and MATRIX

#vector ......its nothing but same as normal vecctors.....it has a magnitude and a direction which will be more than 1 number

vector = torch.tensor([7,7])
vector

vector.ndim

vector.shape
#number of items in the vector element inside the [ ]

#matrix

MATRIX = torch.tensor([[7,8],[9,10]])
MATRIX

MATRIX.ndim

MATRIX.shape

MATRIX[0]

# the name scalar, vector , matrix are jist traditional names .....but torch.tensor is used to create them
# the data is scalar , vecotr or matrix based on what the torch.tensor has....

#like 7 or any nnumber means its a scalar 0D
#if it has 1 row in an array form then its a vector = [7,8] 1D
#if it has 2 row then its a matrix = [[7,8],[9,10]] 2D
#if its more than 2  not rows but like 3 or more [[[]]] then itsa tensor....3D,4D etc based on number of nested [ ]

#TENSOR

TENSOR = torch.tensor(
    [[[1,2,3],
      [3,6,9],
      [2,4,5]]]
)

TENSOR

TENSOR.ndim

TENSOR.shape

TENSOR[0][1]

TENSOR[0][0][2]

"""#Random Tensors

###Random tensors
why random tensors ?

Random trnsors are important because the way many neural netowrk learn is that they start with full ff random number to get better and adjust those numbers to beter represent the data....

`start with random numbers -> look at data -> update random number - look at data  -> update random number`
"""

#create random tensor of size (3,4)

random_tensor  = torch.rand(3,4)
#rand gives a random number which is 0<x<1
#here it will give 3X4 matrix of random number b.w 0 and 1
random_tensor

random_tensor.shape

# it could have any value D's

random_tensor_2 = torch.rand(1,3,3)
random_tensor_2

#create random tensor with similiar shae to an image tensor

#like if we want to convert a image in tensor data , we need colour channel to convert it .......
#colour channel mwans the number of colour sheets the image will have in individual......like red , blue , green.....
#colour channel can be written at first of at last like the 3 colour channnels

random_image_size_tensor = torch.rand(size=(224,224,3)) # the size is used to be (height, width , colour chanel)
# random_image_size_tensor = torch.rand(size=(3,224,224)) # the size is used to be (colour chanel,height, width)
#the result will be (torch.Size([3,224, 224]), 3) still the 3 of [3,224,224] wil be the colour change
random_image_size_tensor.shape, random_image_size_tensor.ndim

 # the o/p which is (torch.Size([224, 224, 3]), 3)  means ([height , width , colour channels] , 3 ) this 3 means 3D which conssot of height , width and colour channels

torch.rand(size=(3,3))

"""#Zeroes and Ones


"""

#Creat5e a tensor of all zeroes......used to perform masking of data
zeros = torch.zeros(size=(3,4)) # Changed 'torch.zeroes' to 'torch.zeros'
zeros

zeros*random_tensor

#create a tensor of all ones

ones = torch.ones(size=(3,4))
ones

ones*random_tensor

ones.dtype
#default dataype is dtype

"""#Creating  a range of tensors and tensors-like

"""

#if  torch.range() depreciated  then use .arange()
torch.arange(0,10)

one_to_ten = torch.arange(start=1,end=11,step=1) # or torch.arange(1,11) from start to end-1 with gap of 1
one_to_ten

#to convert the arange of tensors into zeros
ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros

"""#Tensor Datatypes

Note : Tensor data type will be the one of the 3 big errors u will rin with putrpch and deep learning

1) tensors not right datatype.
2) tensor not right shape.
3) tensor not on right device.

"""

#Float 32 tensor
# float_32_tensor = torch.tensor([3.0,6.0,9.0],dtype=None)
# float_32_tensor = torch.tensor([3.0,6.0,9.0],dtype=torch.float16) the computer stores 16 bit of memory to this value in memory
#the default is 32 , but why to use 16 instead of 32 ??
# the smaller it is the greater the computation speed but lesser the precision the lesser the memory it take to compute and vice versa
float_32_tensor = torch.tensor([3.0,6.0,9.0],
                               dtype=torch.float32,# datatype of tensor or we can use None too
                               device=None, #what device your tensor is on CPU , GPU or none
                               requires_grad=False) # whether or not to track gradients with this gradient operations

# for example for the 3rd problem is  , ltes say one tensor is on cpy device and one is on gpu so it will show error if we access the ddatayoe with wrogn device

float_32_tensor

float_16_tensor = float_32_tensor.type(torch.float16) # it is used to convert the float 32 tensor into float 16 tensor
float_16_tensor

float_16_tensor*float_32_tensor
# suprisingly we multiplies float32 with float 16 and got no error......and it happens sometimes we get no error

int_32_tensor = torch.tensor([3,6,9],dtype=torch.int32)
int_32_tensor

int_32_tensor*float_32_tensor
#still we got no error and it works....... what if we use long tensor ( which is 64 bit )

int_32_tensor_2 = torch.tensor([3,6,9],dtype=torch.long)
int_32_tensor_2

int_32_tensor_2*float_32_tensor
#it still works.......but note we get error when we use so keep that in mind.......



"""#Gettting information from tensors
1. tensors not right datatype. -> to do get datatype from a tensor can use `tensor.dtype
 2. tensor not right shape. -> to do get shape from a tensor can use `tensor.shape
 3. tensor not on right device. ->to do get device from a tensor can use `tensor.device

"""

#Create a tensor

some_tensor = torch.rand(3,4) # 3 rows 4 columns
some_tensor

print(some_tensor)
print(f"Datatype of tensor : {some_tensor.dtype}")
print(f"Shape of tensor : {some_tensor.shape}")
print(f"Device tensor is on : {some_tensor.device}")

new_rtensor = torch.rand([3,4])
new_rtensor



"""#manupulating tensors ( tensor operations )

Tensor operaitions include
 *Addition
 *Subtraction
 *Multiplication
 *Division
 *Matrix multiplication  
"""

#create a tensro and add 10 to it
tensor = torch.tensor([1,2,3])
tensor + 10

tensor*10

tensor-10

torch.mul(tensor,10)

"""#Matrix multiplication"""

print(tensor, "*", tensor)
print(f"Equals : {tensor*tensor}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor,tensor)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# value = 0
# for i in range(len(tensor)):
#   value += tensor[i]*tensor[i]
# print(value)



"""
There are two main rules that performing matrix multiplication needs to satisfy ;
1.The **inner dimensions** must match:
* `(3,2) @  (3,2)` wont work
* `(2,3) @  (3,2)` will work
* `(3,2) @  (2,3)` also work

"""

#  torch.matmul(torch.rand(3,2),torch.rand(3,2)) gives error

torch.matmul(torch.rand(2,3),torch.rand(3,2))

"""#One of the most common errrors in deep learning : SHAPE ERROR

"""

#Shapes for matrix multiplication

tensor_A = torch.tensor([[1,2],
                        [3,4],
                        [5,6]])

tensor_B = torch.tensor([[7,8],
                        [9,10],
                        [11,12]])

# torch.mm(tensor_A,tensor_B) error

tensor_A.shape

"""To fix our tensor shape issues , we can manuluplate the shape of one our tensors using transponse

A **transpose** switches the axes or dimension of a given tensor
"""

tensor_B.T #it transposes the matrix tensor_B

tensor_A.shape,tensor_B.shape

tensor_A.T.shape,tensor_B.T.shape

torch.mm(tensor_A,tensor_B.T)

print(f"the output shape : {torch.mm(tensor_A,tensor_B.T).shape}")



"""#Finding min, max, mean , sum etc ( tensor aggregation)"""

x= torch.arange(0,100,10)

torch.min(x) , x.min()

torch.max(x) , x.max()

# torch(x) # gives error as datatype should be float or etc as tensor is not a right datatype
torch.mean(x.type(torch.float32)) , x.type(torch.float32).mean()

torch.sum(x) , x.sum()

"""#Finding positional min and max"""

x.argmin()

x[0]

x.argmax()

x[9]



"""\

## Reshaping , stacking , squeezing and unsqueezing tensors

* Reshaping - reshapes an input tensor toa  defined shape
* view - return a view of an input tensor of certain shape but keep the same memory as the original tensor
*Stacking - combined multiple tensors on top of each other ( Vstack ) or Hstack for side by side and  etc
*squeeze - removes all '1' dimensions from a tensor
*unsqueeze- add a `1` dimension to a target tensor
*permute - return a view of the input with dimesnsion permuted ( swapped) in a  certain way
"""

y = torch.arange(1.0,10.0)
y,y.shape

# y_reshaped = y.reshape(1,6) #error bcz we are trying to squeeze 9 elements into 6 elements
y_reshaped = y.reshape(1,9)
y_reshaped,y_reshaped.shape

# y_reshaped = y.reshape(2,9) error bcz we dont have 2*9 i.e 18 elements
y_reshaped = y.reshape(9,1) # also 9*1=9 run bcz we have 9 elements and number of rows can are 9 with each having 1 element
y_reshaped

y_reshaped = y.reshape(3,3) # works as 3*3=9
y_reshaped

#change the view

z= y.view(1,9)
z,z.shape

#change in z changes c ( bcz a view of a tensor shares the same memory as original tensor)

z[:,0]=5
z,y

#Stacking tensors on top of each other
torch.stack([y,y,y,y])

torch.stack([y,y,y,y],dim=0)

torch.stack([y,y,y,y],dim=1)

torch.stack([y,y,y,y],dim=2)

y_reshaped

#Squeeze

torch.squeeze(y_reshaped) , y_reshaped.squeeze()

torch.squeeze(y_reshaped).shape

y_reshaped= y_reshaped.reshape(1,9)
y_reshaped.shape

y_reshaped.squeeze() # removed 1D from [1,9] and if its [1,1,3] it will remove all 1D's i.e [3] is final

print(f"Previous tensor : {y_reshaped}")
print(f"Previous shape : {y_reshaped.shape}")

#remove extra dimension from y_reshaped

#unsqueeze
y_unsqueezed = y_reshaped.unsqueeze(dim=0)
y_unsqueezed

print(f"New tensor : {y_reshaped.shape}")
print(f"New tensor : {y_unsqueezed}")
print(f"New shape : {y_unsqueezed.shape}")

y_unsqueezed_dim_1 = y.squeeze().unsqueeze(dim=1)
  y_unsqueezed_dim_1

print(f"New tensor : {y_reshaped.shape}")
print(f"New tensor : {y_unsqueezed_dim_1}")
print(f"New shape : {y_unsqueezed_dim_1.shape}")

a = torch.randn(2, 3, 5)
a.shape

a.permute(2, 0, 1).shape

"""#Selecting data from tenors ( indexing )"""

b = torch.rand(1,3,3)
b

b[0][0]

b[:,0] # get all values of 0th dimesion
# so the : will get all the elements of outer [ ] i.e [[0.1219, 0.3639, 0.1201],
         #[0.0824, 0.4818, 0.2073],
         #[0.3832, 0.6041, 0.1817]]

# now 0 will give us the the [ ] with 0th index....i.e  [0.1219, 0.3639, 0.1201]
# if we said [ : , : , 0] means now from   [0.1219, 0.3639, 0.1201]  we want the  item with 0th index i.e 0.1219

b[: ,: , 1] #get all values of 0th and 1st dimeension but only index 1 of 2nd dimesions

b[:, 1 , 1] # is same as b[0][1][1]....but : means all not 0

b[0 , 0 , :]

"""#PyTorch Tensors and NumPy

NumPy is a  popular scientific python numerical computing library

And beacuse of this , PyTorch has functionality to interact with it

* Data in NumPy , want in PyTorch tensor -> `torch.from_numpy(ndarray)`
*PyTorch tensor -> NumPy -> ` torch.Tensor.numpy()`

"""

np_array = np.arange(1.0,8.0)
tensor = torch.from_numpy(np_array)
tensor,tensor.dtype, np_array , np_array.dtype

# when we create a tensor from numpy array we get a new tensor with new memory ...the tensor and that numpy array doesnt share the same memory

torch.arange(1.0,8.0).dtype

# so we can see the dtype of arange from np and torch is different.....
# np create float64 as default dtype while torch creates a float32 as default .....
# so when we convert np array into tensor we need to change the type of the array first before converting them or error may rise

#so when we change the array , the tensor doesnt changes
np_array+1 , tensor

# Tensor to numpy array....

tensor = torch.ones(7)
numpy_tensor = tensor.numpy() # changes tensor into numpy array.....the numpy created from tensor will have float32 dtype
tensor , tensor.dtype , numpy_tensor

tensor= tensor+1
tensor , numpy_tensor
# they also dont share memories

"""##Reproductibility ( trying to take random out of random )

In short how a neural network learns :

`start with random numbers - > tensor operations -> update numbers to try to make them better representation of the data -> again -> again -> again.....`

so we want to reduce thid RANDOMNESS OF IT.....to reduce it here comes the concept of RANDOM SEED in pytorch
essentially what the random seed does is "flavor: the randomness

"""

torch.rand(3,3) # every single time we run it we get RANDOM NUMBERS everytime.....
# so we want to reduce thid RANDOMNESS OF IT.....

random_tensor_A = torch.rand(3,4)
random_tensor_B = torch.rand(3,4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

# so lets say we share this thing to someone and they get the different values and somehow they get true in all indexes so it would be wrong

#lets make some random but reproducible ( which can be made again when called ) tensors

#set random seed

RANDOM_SEED = 42 # any random number.....42 bcz ITS THE BEST OUT NUMBER WHICH ANSWERS UNIVERSAL CALLS
torch.manual_seed(RANDOM_SEED) # when we use random seed it will call for 1  BLOCK of code everytime.....and then resets for other blocks so we have to call it everytime
random_tensor_C = torch.rand(3,4)

torch.manual_seed(RANDOM_SEED) # we recalled it bcz it was used for random_tensor_C and we need it again for D
random_tensor_D = torch.rand(3,4)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)

torch.manual_seed(RANDOM_SEED)
random_tensor_E = torch.rand(3,4)
print(random_tensor_E)

"""## running tensors and pyotch objects on the GPU;s and making them faster

GPU = faster computation on numbers , thanks to CUDA + NVIDIA h/w + PyTorch BTS to make everything make good

# Getting a GPU
1.Easiest - use google collab for a free GPU ( options to upgrade as well )

2.use yoour own GPU - take a litle bit of setup and requires the investment of purchasing a a GPU , there is lots of options

3.use cloud computing

2.Check for Gpu acess with Pytroch
"""

torch.cuda.is_available()

device = "cuda" if torch.cuda.is_available() else "cpu"
device

#count number of devices
torch.cuda.device_count()

"""## Putting tensor and models on the GPU

the reason we want out tensor/models on the GPU is using a GPU result in faster in computations
"""

tensor = torch.tensor([1,2,3])

print(tensor,tensor.device)

# move tensor to GPU if available

tensor_on_gpu = tensor.to(device)
tensor_on_gpu , tensor_on_gpu.device

"""#moving tensors back to cpu"""

#if tensor is on gpu cant transform it into numpy

tensor_on_gpu.numpy() # it worked bcz tensor_on_gpu was on CPU from start

#convert it from GPU to CPU do

tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu

