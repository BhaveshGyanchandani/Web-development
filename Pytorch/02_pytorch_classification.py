# -*- coding: utf-8 -*-
"""02_pytorch_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LEtPrtokl1Mio0vktIDk2RDjaiOallMF

#Neural network classification with pytroch

classificaation is a prooblem of predicting whether simethung is one thing or another ( there can be multiple things as the option)

#1, make some classificaion data
"""

from sklearn.datasets import make_circles
import pandas as pd
import matplotlib.pyplot as plt
import torch
from torch.nn import BCEWithLogitsLoss
#make 1000 samples

n_samples = 1000

#create circles
X,y = make_circles(n_samples,
                   noise = 0.03,
                   random_state=42)

len(X) , len(y) # like features and labels

print(f"first 5 samples of X: {X[:5]}")
print(f"first 5 samples of y: {y[:5]}")

#we have 2 features corresponding with 1 LABEL
                #feature1(x1)  feature2(x2)
#so the sample [ 0.75424625  0.23148074] has label of 1 aand [ 0.44220765 -0.89672343] has label of 0 and other are like this accordingly

#make a DataFrame of circle data

circles = pd.DataFrame({"X1 ": X[:,0],
                       "X2 ": X[:,1],
                       "label" :y})

circles.head(10)

#visulaize
plt.scatter(x=X[:,0],
           y=X[:,1],
           c=y,
           cmap=plt.cm.RdYlBu)

"""Note : The data we are working is often  referred as toy dataset , a dataset is small enough but stil sizeable enough to prsactice fundamentals

#1.1 check input and output shapes
"""

X.shape , y.shape
#(1000,2) means 1000 samples and has 2 features while y has 1000 samples but no features its just a simgle numbve or scalar

X

#View the first example of features and labels
X_sample = X[0]
y_sample = y[0]
print(f"X_sample : {X_sample}")
print(f"y_sample : {y_sample}")

"""#1.2 turn data into tenspr and create train and test splits"""

X = torch.from_numpy(X).type(torch.float)
# X = X.type(torch.float)
y = torch.from_numpy(y).type(torch.float)

X[:5] , y[:5]

#split data into training and test data

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,
                                                  y,
                                                  test_size=0.2,#20% of data will be for test and 80% for train
                                                  random_state=42)

X_train.shape , X_test.shape , y_train.shape , y_test.shape

X_train[:5] , y_train[:5]

"""#2.Builfing a model
to classify our blue and red dots

To do so we ant to :
1.setup device agnostic code so our code will run on GPU if there is one
2.construct a model (by subclasssing nn.Module)
3.define a loss function and optimizer
4.create a traimimg amd test loop
"""

import torch
from torch import nn

#make device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

"""#lets create a model that :

1.Subclasses `nn.Module`
2.Create 2 `nn.linear()` layers that are capable of handlung shapes of our data
3.Defines a `forward()` method that outlines the forward pass ( or forward computation )  
4.instantiate an instance of our model class and send it to the target device

"""

#1.construct a model that subclasses nn.Module

class CircleModelV0(nn.Module):
  def __init__(self):# its a constructor
    super().__init__()

    #2. create 2 nn.linear layers capable of handlig the shpes of our data....
    self.layer_1 = nn.Linear(in_features=2, # 2 as X has 2 features .....and there are 800 samples and each has 2 features i.e [0.9922 0.45747]
                            #  out_features=1) # we want to map 1 sample of X (having 2 features) with 1 sample of y
                                out_features=5) # so basically it takes 2 features and upscales to 5 feeatures

    # it will tak 2 features of X and perform nn.Linear function on it ( see the formula).....and then upscale it to 5 features
    #the more the hidden features the more the possibility of our model to learn more about hidden data ......so at begin it will learn on 2 feature then it will learn upto 5

    #but note that the shape of what is going out in layer_1 should be same shape as what is goinng "in" the layer_2

    self.layer_2 = nn.Linear(in_features=5, # in_feature of next layer should match the out_feature of previous layer
                             out_features=1) #takes in 5 feature from previous layer and outputs a single feature in output layer ( in same shape as y )

    #using it we can make layers well easily.....
    # self.two_linear_layers = nn.Sequential(
    #                 nn.Linear(in_features=2 ,  out_features=5),
    #                 nn.Linear(in_features=5 ,  out_features=1)
    #                 )


  #3. define a forward() method that outline our forward pass
  def forward(self,x): # it takes x as input as some form of data.....
    return self.layer_2(self.layer_1(x))
    # return self.two_linear_layers(x)

    # x goes into layer 1 , the o.p of layer_1 goes into layer 2 and then layer_2's o.p gets returned as output

#4. instantiate tan instance of our model class and send it to the target device

model_0  =CircleModelV0().to(device)
model_0

#lets replicate the model above
model_0 = nn.Sequential(
   nn.Linear(in_features=2 ,  out_features=5),
   nn.Linear(in_features=5 ,  out_features=1)
).to(device)

model_0

model_0.state_dict()
# its like weight has 2*5 = 10 samples

#make predictions

with torch.inference_mode():
  untrained_preds = model_0(X_test.to(device))

print(f"untrained_preds : {untrained_preds[:10]}")
print(f"y_test : {y_test[:10]}")

"""#Setting up loss function and optimizer

we will use cross entropy loss for multi class clssification and binary cross entropy for binary class classsfification
"""

# setup loss funciton
#
# loss_fn = nn.BCELoss() # BCELoss = rewuires inputs to have gone throuhh thw signmoid activaton funciton proior ti the input to BCELoss.....its like below
  # nn.Sequential(
  #     nn.Sigmoid(),
  #     nn.BCELoss()
  # )

loss_fn=BCEWithLogitsLoss() # has sigmoid activation function built-in

optimizer = torch.optim.SGD(params=model_0.parameters(), # optimizes model_0.parameters such that loss can go down using loss_fun
                            lr = 0.01)

#calculate accuracy...which is.....true_positive/(true_positive+true_negative)*100

# like calculate accuracy - out of 100 samples  ,what percentage our model gets right ?

def accruacy_fn(y_true,y_pred):
  correct = torch.eq(y_true,y_pred).sum().item() # torch.eq() calculates how many of our y_true and y_pred samples are equal then sum them
  acc = (correct/len(y_pred))*100
  return acc

"""#3.Train our model with that 5 steps

#going raw logits - > prediction probabilities -> prediction tables

our model outputs are going to be raw `logits`

we can convert these **logits** into prediction probabilities by passing them to some kinf of activation funciton ( eg. sigmoid for biinart classification and softmax for multi-class classification)

Then we can convert our model prediction probabilities to prediction labels by either rounding the, or taking the `argmax()`
"""

# view the first 5 output s of thr forward pass on the test data

with torch.inference_mode():
  y_logits = model_0(X_test.to(device))

y_logits[:5]

#use the sigmoid aactiviation function on our logits ....which is ht eoutput activation function ......on our model to turn them into prediciotn probalities
y_pred_probs = torch.sigmoid(y_logits) # now we can pass them to torch.round function
y_pred_probs[:5]

"""FOr our prediciton probabilty values , we need to performa range-style rounfing on them ⁉
*`y_preds_probs`>= 0.5 , `y=1` (class 1)

*`y_preds_probs`< 0.5 , `y=0` (class 0)
"""

torch.round(y_pred_probs[:5])

# find the predicted labels

y_preds = torch.round(y_pred_probs)

#in full  (logits -> pred probs -> pred labels)
# convetrted logits into pred probs using sigmoid and usinf round we cnverted pred pdobs into pred labels
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))))

#check for equality
print(torch.eq(y_preds.squeeze(),y_pred_labels.squeeze())) # Changed squeezze() to squeeze()

#get rif og extra dimension
y_preds.squeeze()

y_test[:5]

"""#3.2 Builing a training and testing loop"""

torch.manual_seed(42)

epochs = 200

#put data to target device

X_train , y_train, X_test , y_test = X_train.to(device) , y_train.to(device) , X_test.to(device) , y_test.to(device)

#Build training and evaluation loop

for epoch in range (epochs):
  #training
  model_0.train()

  #1.forward pass
  y_logits = model_0(X_train).squeeze() # squeezw to get rid of extra 1 dimensioin from tensor

  y_pred = torch.round(torch.sigmoid(y_logits))

  # or we can use  y_pred = torch.round(torch.sigmoid(model_0(X_train).squeeze() )

  #2. calculate loss and accuracy ( well not necessary )

  # if loss function was BCELoss then woulf expect prediction probabilities as inputs....
  #loss =loss_fn(torch.sigmoid(y_logits),y_train)

  loss=loss_fn(y_logits,y_train) # compares y_logits with y_train and gives the loss .....nn.BCEWithlogitsLoss expects raw logits as inputs..whcih is basically PREDUCTION , TRUE LABELS comparision
  acc = accruacy_fn(y_true =y_train, y_pred=y_pred)


  #3. optimizer zero grad

  optimizer.zero_grad()

  #4.loss backpropagation
  loss.backward()

  #5. optimizer step
  optimizer.step()

  #testing
  model_0.eval()
  with torch.inference_mode():

    #1.forawed pass
    test_logits = model_0(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    #2.calculate test loss and accuracy
    test_loss = loss_fn(test_logits,y_test)
    test_acc = accruacy_fn(y_true=y_test,y_pred=test_pred)

  #print whats happening

  if epoch % 20 == 0:
    print(f"Epoch : {epoch} | loss : {loss:.5f} | acc : {acc:.2f}% | test_loss : {test_loss} | test_acc : {test_acc:.2f}%")

"""# 4. Make prediuctions and evvalutate the model


from the metrcis it looks like our model isnt learnign anything


so to inspea=ct it lets make some predicctions and make them visual

in other words VISUALIZE.

To do so we ware going to inport a funcion called `plot_decision_boundary`
"""

import requests

from pathlib import Path

#Downlaod heloer fntions form learn Pytotch rrepo (if tis not already donwlaoded)

if Path("helper_functions.py").is_file():
  print("helper_funcitns.py already exist , skipping download")
else:
  print("downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")

  with open("helper_functions.py","wb") as f:
    f.write(request.content)

from helper_functions import plot_decision_boundary ,plot_predictions

#plot decisionn boundary of the mdodel

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_0,X_train,y_train)
plt.subplot(1,2,2)
plt.title("Test")
plot_decision_boundary(model_0,X_test,y_test)

"""#5.Improving a model

* Add more layers  - give the model more chances to learn about patterns in data

* Add more hidden units- go from 5 hidden units to 10 hidden units

* Fit for longer

* Changing the activation function

* change the learning rate ( too large might explode the gradients )

* change the loss function

These optoins are all from a models perspective because they deal directly with the model , rather than the data

And because these options are all values we ( as ML engineers and data scientists ) can change , they are referred as **hyperParameters**

Lets try and improve our model by ⁉
  * adding more hideen units : 5->10
  * inc the number of layer : 2 -> 3
  * inc epochs : 100 - > 1000
"""

class CircleModelV1(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2,out_features=10)
    self.layer_2 = nn.Linear(in_features=10,out_features=10)
    self.layer_3 = nn.Linear(in_features=10,out_features=1)

  def forward(self,x):
    return self.layer_3(self.layer_2(self.layer_1(x)))

model_1 = CircleModelV1().to(device)
model_1

model_1.state_dict()

loss_fn = nn.BCEWithLogitsLoss()

optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr=0.1)

torch.manual_seed(42)

epochs = 1000

#put data to target device

X_train , y_train, X_test , y_test = X_train.to(device) , y_train.to(device) , X_test.to(device) , y_test.to(device)

#Build training and evaluation loop

for epoch in range (epochs):
  #training
  model_1.train()

  #1.forward pass
  y_logits = model_1(X_train).squeeze()

  y_pred = torch.round(torch.sigmoid(y_logits))

  #2. calculate loss and accuracy ( well not necessary )

  loss=loss_fn(y_logits,y_train)
  acc = accruacy_fn(y_true =y_train, y_pred=y_pred)


  #3. optimizer zero grad

  optimizer.zero_grad()

  #4.loss backpropagation
  loss.backward()

  #5. optimizer step
  optimizer.step()

  #testing
  model_1.eval()
  with torch.inference_mode():

    #1.forawed pass
    test_logits = model_1(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    #2.calculate test loss and accuracy
    test_loss = loss_fn(test_logits,y_test)
    test_acc = accruacy_fn(y_true=y_test,y_pred=test_pred)


  if epoch % 100 == 0:
    print(f"Epoch : {epoch} | loss : {loss:.5f} | acc : {acc:.2f}% | test_loss : {test_loss} | test_acc : {test_acc:.2f}%")

#plot decisionn boundary of the mdodel

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_1,X_train,y_train)
plt.subplot(1,2,2)
plt.title("Test")
plot_decision_boundary(model_1,X_test,y_test)

"""#5.1 Preparing data to see if out model can fit a straight line

one way to troubleshoot to a larger problem is to test out a smaller problem
"""

weight = 0.7
bias = 0.3

start=0
end=1
step=0.01


X_regression = torch.arange(start,end,step).unsqueeze(dim=1)
y_regression = weight * X_regression + bias

train_split = int(0.8 * len(X_regression))

X_train_reg = X_regression[:train_split]
y_train_reg = y_regression[:train_split]

print(len(X_regression),len(X_train_reg),len(y_train_reg))

X_test_reg = X_regression[train_split:]
y_test_reg = y_regression[train_split:]

print(len(X_test_reg))



#create train and test splits

train_split = int(0.8 * len(X_regression))

X_train_regression = X_regression[:train_split]
y_train_regression = y_regression[:train_split]

print(len(X_regression),len(X_train_regression),len(y_train_regression))

X_test_regression = X_regression[train_split:]

y_test_regression = y_regression[train_split:]

print(len(X_test_regression),len(y_test_regression))

plot_predictions(train_data=X_train_regression,
               train_labels=y_train_regression,
               test_data=X_test_regression,
               test_labels=y_test_regression,
               predictions=None);

X_train_regression[:10]

model_1 # but this time we only have 1 feature per sample .....how do we know ?? we use X_train_regression.....
# so we have to adjust infeatures to 1

"""#5.2 Adjusting `model_1` to fit a straight line"""

#same architecture

model_2 =nn.Sequential(
    nn.Linear(in_features=1,out_features=10),
    nn.Linear(in_features=10,out_features=10),
    nn.Linear(in_features=10,out_features=1)
).to(device)

model_2

#loss and optimizwr

loss_fn = nn.L1Loss() # bcz we dealin with regresssion now as it is straight line not an classifiaction problem

optimizer = torch.optim.SGD(params=model_2.parameters(),
                            lr=0.1)

#train it on train dataset and test it

torch.manual_seed(42)

epochs= 1000

X_train_regression , y_train_regression = X_train_regression.to(device) , y_train_regression.to(device)

X_test_regression, y_test_regression = X_test_regression.to(device) , y_test_regression.to(device)

for epoch in range(epochs):
  model_2.train()

  #1.forward pass
  y_pred = model_2(X_train_regression)
  loss = loss_fn(y_pred , y_train_regression)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  #testing

  model_2.eval()
  with torch.inference_mode():
    test_pred = model_2(X_test_regression)
    test_loss = loss_fn(test_pred,y_test_regression)

    #print whats happeining

    if epoch%100 == 0 :
      print(f"Epoch : {epoch} | loss : {loss:.5f} | test_loss : {test_loss}")

# so now we saw our model is learning something since loss is going down

#Turn on evaluation mode

model_2.eval()

#make prediction

with torch.inference_mode():
  y_preds = model_2(X_test_regression)

  #plot the data and predictions
  # Changed train_label to train_labels, and test_label to test_labels
  plot_predictions(train_data = X_train_regression ,
                  train_labels = y_train_regression ,
                   test_data = X_test_regression ,
                   test_labels = y_test_regression,
                   predictions = y_preds
                   );

"""#6. The missing piece : non-linearity

"what patterns could you draw if u weere given an infinite amount of a straight and non straight lines ?"

or in a ML terms , an infinite but really its finite , of linear and non lineaer fnctions

#6.1 Recreating non - linear data (red and blue circles )
"""

import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

n_samples = 1000

X,y = make_circles(
    n_samples,
    noise=0.03,
    random_state=42
)

plt.scatter(X[:,0],X[:,1],c=y,cmap=plt.cm.RdYlBu)
#cmap means colour map , cmap red yellow blue

#convert data to tensor and then train and test split

import torch
from sklearn.model_selection import train_test_split

#turn data into tensors bcz we use tensors bcz we use tensor to train and test

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

#split into train and test set

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

print(len(X_train),len(X_test))

"""##6.2 Building a model with non linearity

"""

X_train[:5]

#Builf a model with non linear activation formulaa

from torch import nn
class CircleModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2,out_features=10) # we use 2 bc X_train has 2 features in each sample.....[ ] is called a sample and [ [],[],...[]  ] is called colectioin of samples
    self.layer_2 = nn.Linear(in_features=10,out_features=10)
    self.layer_3 = nn.Linear(in_features=10,out_features=1)
    self.relu = nn.ReLU() # our activation function , non linear activation function

    # relu takes an argument , and makes the o.p 0 if the o.p return by relu finction is -ve and doesnt do anything whenn the o.p returned by function is positive and returns the positive value as it is.....

  def forward(self,x):
    return self.layer_3(self.relu(self.layer_2(self.relu((self.layer_1(x))))))

model_3 = CircleModelV2().to(device)
model_3

##loss function and optimizer

loss_fn = nn.BCEWithLogitsLoss()

#optimiser

optimizer = torch.optim.SGD(params = model_3.parameters(),
                            lr=0.1)

"""#6.3 training a model with non linearity


"""

#random seed
torch.manual_seed(42)


#device agnostic code
X_train , y_train, X_test , y_test = X_train.to(device) , y_train.to(device) , X_test.to(device) , y_test.to(device)

epochs = 1000

for epoch in range(epochs):
  model_3.train()

  #forward pass
  y_logits = model_3(X_train).squeeze() # logits -> prediction probabilities -> prediction labels
  y_preds = torch.round(torch.sigmoid(y_logits))
  loss  = loss_fn(y_logits , y_train ) #BCEWithLogitsLoss takes logits as first input

  #loss
  loss = loss_fn(y_logits,y_train)
  acc = accruacy_fn(y_true=y_train,y_pred=y_preds)

  #backpropogation
  optimizer.zero_grad()

  loss.backward()

  optimizer.step()

  #testing
  model_3.eval()
  with torch.inference_mode():
    test_logits = model_3(X_test).squeeze()
    test_loss = loss_fn(test_logits ,y_test )
    test_pred = torch.round(torch.sigmoid(test_logits))
    test_acc = accruacy_fn(y_true=y_test,y_pred=test_pred)

  if epoch % 100 == 0:
    print(f"Epoch : {epoch} | loss : {loss:.5f} | acc : {acc:.2f}% | test_loss : {test_loss} | test_acc : {acc:.2f}%")

#visualize

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_3,X_train,y_train)
plt.subplot(1,2,2)
plt.title("Test")
plot_decision_boundary(model_3,X_test,y_test)

"""#7.Replicatig non linear activation functins"""

#creating a tensor

A = torch.arange(-10 ,10 ,1 ,dtype=torch.float32)
A

plt.plot(A)

plt.plot(torch.relu(A))



"""#there are 2-3 section remainning as daniel did model training on other type od datatypes too......"""

